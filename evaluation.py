from dataset import *

import faiss
import scipy.spatial
from torch import Tensor

def get_hits_slow(em1, em2, test_pair, top_k=(1, 10)):
    em1 = em1.detach().numpy()
    em2 = em2.detach().numpy()
    Lvec = np.array([em1[e1] for e1, e2 in test_pair])
    Rvec = np.array([em2[e2] for e1, e2 in test_pair])
    sim = scipy.spatial.distance.cdist(Lvec, Rvec, metric='cityblock')
    top_lr = [0] * len(top_k)
    for i in range(Lvec.shape[0]):
        rank = sim[i, :].argsort()
        rank_index = np.where(rank == i)[0][0]
        for j in range(len(top_k)):
            if rank_index < top_k[j]:
                top_lr[j] += 1
    top_rl = [0] * len(top_k)
    for i in range(Rvec.shape[0]):
        rank = sim[:, i].argsort()
        rank_index = np.where(rank == i)[0][0]
        for j in range(len(top_k)):
            if rank_index < top_k[j]:
                top_rl[j] += 1
    print('For each left:')
    for i in range(len(top_lr)):
        print('Hits@%d: %.2f%%' % (top_k[i], top_lr[i] / len(test_pair) * 100))
    print('For each right:')
    for i in range(len(top_rl)):
        print('Hits@%d: %.2f%%' % (top_k[i], top_rl[i] / len(test_pair) * 100))



def my_dist_func(L, R, k=100):
    dim = len(L[0])
    print(f"åˆå§‹åŒ–FAISSç´¢å¼•ï¼Œç»´åº¦: {dim}, æŸ¥è¯¢æ•°: {len(L)}, ç´¢å¼•æ•°: {len(R)}")
    torch.cuda.empty_cache()
    # faiss.normalize_L2(L)
    # faiss.normalize_L2(R)
    index = faiss.IndexFlatIP(dim)
    # index = faiss.index_cpu_to_all_gpus(index)
    print(f"æ·»åŠ  {len(R)} ä¸ªå‘é‡åˆ°ç´¢å¼•...")
    index.add(R)
    print(f"æ‰§è¡Œ {len(L)} ä¸ªæŸ¥è¯¢ï¼Œtop_k={k}...")
    D, I = index.search(L, k)
    print("FAISSæœç´¢å®Œæˆ")
    return D,I

def get_hits(em1, em2, test_pair, top_k=(1, 5, 10, 50, 100), partition=1, norm=False, src_nodes=None, trg_nodes=None):
    if isinstance(em1, Tensor):
        em1 = em1.cpu().detach().numpy()
        em2 = em2.cpu().detach().numpy()
    if norm:
        # em1= norm_process(torch.from_numpy(em1)).detach().numpy()
        # em2= norm_process(torch.from_numpy(em2)).detach().numpy()
        em1 = em1 / np.linalg.norm(em1, axis=-1, keepdims=True)
        em2 = em2 / np.linalg.norm(em2, axis=-1, keepdims=True)

    def filter_pair(pair, src, trg):
        if src is None or trg is None:
            return pair
        src = set(src)
        trg = set(trg)
        return list(filter(lambda x: x[0] in src and x[1] in trg, pair))

    batch_size = len(test_pair) // partition
    print(batch_size)
    total_size = 0
    top_lr = [0] * len(top_k)
    for x in range(partition):
        left = x * batch_size
        right = left + batch_size if left + batch_size < len(test_pair) else len(test_pair)
        filtered = filter_pair(test_pair[left:right], src_nodes, trg_nodes)
        print(len(filtered))
        if len(filtered) == 0:
            continue
        total_size += len(filtered)
        Lvec = np.array([em1[e1] for e1, e2 in filtered])
        Rvec = np.array([em2[e2] for e1, e2 in filtered])
        ranks = my_dist_func(Lvec, Rvec)
        for i in range(Lvec.shape[0]):
            rank = ranks[i]
            rank_index = np.where(rank == i)[0][0] if i in rank else 1000
            for j in range(len(top_k)):
                if rank_index < top_k[j]:
                    top_lr[j] += 1
    print('For each left:')
    print('Total size=', total_size)
    for i in range(len(top_lr)):
        str = 'Hits@%d: %.2f%%' % (top_k[i], top_lr[i] / (total_size + 1e-8) * 100)
        with open('hits.txt', 'a+') as f:
            f.write(str + '\n')
            f.close
        print(str)

    return top_k, top_lr, total_size
    
def test(test_pair, features, top_k=200, iteration=15, min_precision=1e-12, temperature=0.02):
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šç¡®ä¿Sinkhornè¿­ä»£ç¨³å®šè¿è¡Œ
    
    Args:
        test_pair: æµ‹è¯•å¯¹
        features: ç‰¹å¾çŸ©é˜µ
        top_k: å€™é€‰æ•°é‡
        iteration: Sinkhornè¿­ä»£æ¬¡æ•°
        min_precision: æœ€å°ç²¾åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼ä¼šè¢«è®¾ä¸ºæ­¤å€¼
        temperature: æ¸©åº¦å‚æ•°
    """
    
    # å†…å­˜æ¸…ç†
    print("ğŸ§¹ æ¸…ç†å†…å­˜å’Œæ˜¾å­˜...")
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print("âœ“ PyTorch GPUç¼“å­˜å·²æ¸…ç†")
    except ImportError:
        pass
    
    try:
        import tensorflow as tf
        tf.keras.backend.clear_session()
        tf.compat.v1.reset_default_graph() if hasattr(tf.compat.v1, 'reset_default_graph') else None
        print("âœ“ TensorFlow GPUå†…å­˜å·²æ¸…ç†")
    except (ImportError, AttributeError):
        pass
     
    import gc
    gc.collect()
    print("âœ“ Pythonåƒåœ¾å›æ”¶å®Œæˆ")
    
    left, right = test_pair[:,0], np.unique(test_pair[:,1])
    print(f"Testå‡½æ•° - left: {len(left)}, right: {len(right)}, top_k: {top_k}")
    
    # è·å–ç‰¹å¾
    features_l = features[left]
    features_r = features[right]
    print(f"ç‰¹å¾ç»´åº¦ - left: {features_l.shape}, right: {features_r.shape}")
    
    # å½’ä¸€åŒ–
    faiss.normalize_L2(features_l)
    faiss.normalize_L2(features_r)
    
    # åŠ¨æ€è°ƒæ•´top_kï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„å€™é€‰
    min_candidates = max(50, len(np.unique(test_pair[:,1])) // 10)  # è‡³å°‘50ä¸ªå€™é€‰æˆ–ç›®æ ‡æ•°é‡çš„10%
    safe_top_k = min(max(top_k, min_candidates), len(features_r))
    
    if safe_top_k < top_k:
        print(f"âš ï¸ top_kè°ƒæ•´: {top_k} -> {safe_top_k} (å—é™äºå€™é€‰æ•°é‡)")
    elif safe_top_k > top_k:
        print(f"ğŸ“ˆ top_kæ‰©å±•: {top_k} -> {safe_top_k} (ç¡®ä¿è¶³å¤Ÿå€™é€‰)")
    
    print(f"æœ€ç»ˆtop_k: {safe_top_k}")
    
    # FAISSæ£€ç´¢
    dim = features_l.shape[1]
    index = faiss.IndexFlatIP(dim)  
    index.add(features_r)
    sims, indices = index.search(features_l, safe_top_k)
    
    print("å¼€å§‹ç¨³å®šç‰ˆSinkhornè¿­ä»£...")
    
    # è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå¹¶åº”ç”¨ç²¾åº¦ä¿æŠ¤
    raw_sims = sims.flatten()
    
    # æ£€æŸ¥å¹¶ä¿®å¤ç²¾åº¦é—®é¢˜
    low_precision_mask = np.abs(raw_sims) < min_precision
    if np.any(low_precision_mask):
        num_low = np.sum(low_precision_mask)
        print(f"âš ï¸ å‘ç° {num_low} ä¸ªä½ç²¾åº¦å€¼ (< {min_precision})ï¼Œè¿›è¡Œä¿®æ­£...")
        raw_sims[low_precision_mask] = np.sign(raw_sims[low_precision_mask]) * min_precision
    
    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    row_sims = np.exp(raw_sims / temperature)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
    if np.any(np.isnan(row_sims)) or np.any(np.isinf(row_sims)):
        print("âš ï¸ å‘ç°NaNæˆ–Infå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
        row_sims = np.nan_to_num(row_sims, nan=min_precision, posinf=1.0, neginf=min_precision)
    
    flat_indices = indices.astype(np.int32).flatten()
    
    size = len(left)
    total_elements = size * safe_top_k
    
    # åˆ›å»ºç´¢å¼•
    row_ids = np.repeat(np.arange(size), safe_top_k)
    
    print(f"Sinkhornå‚æ•°:")
    print(f"  - è¿­ä»£æ¬¡æ•°: {iteration}")
    print(f"  - æ¸©åº¦å‚æ•°: {temperature}")
    print(f"  - æœ€å°ç²¾åº¦: {min_precision}")
    print(f"  - çŸ©é˜µå¤§å°: {size} x {len(features_r)}")
    print(f"  - éé›¶å…ƒç´ : {total_elements}")
    
    # ç¨³å®šçš„Sinkhornè¿­ä»£
    convergence_threshold = 1e-6
    prev_row_sims = row_sims.copy()
    
    for iter_num in range(iteration):
        # è¡Œå½’ä¸€åŒ– - æ·»åŠ ç¨³å®šæ€§æ£€æŸ¥
        row_sums = np.bincount(row_ids, weights=row_sims, minlength=size)
        
        # é˜²æ­¢é™¤é›¶
        row_sums = np.maximum(row_sums, min_precision)
        row_normalizers = row_sums[row_ids]
        row_sims = row_sims / row_normalizers
        
        # åˆ—å½’ä¸€åŒ– - æ·»åŠ ç¨³å®šæ€§æ£€æŸ¥  
        col_sums = np.bincount(flat_indices, weights=row_sims, minlength=len(features_r))
        
        # é˜²æ­¢é™¤é›¶
        col_sums = np.maximum(col_sums, min_precision)
        col_normalizers = col_sums[flat_indices]
        row_sims = row_sims / col_normalizers
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if np.any(np.isnan(row_sims)) or np.any(np.isinf(row_sims)):
            print(f"âš ï¸ è¿­ä»£ {iter_num + 1}: å‘ç°æ•°å€¼ä¸ç¨³å®šï¼Œè¿›è¡Œä¿®æ­£")
            row_sims = np.nan_to_num(row_sims, nan=min_precision, posinf=1.0, neginf=min_precision)
        
        # æ”¶æ•›æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
        if iter_num > 0 and iter_num % 5 == 0:
            diff = np.mean(np.abs(row_sims - prev_row_sims))
            print(f"  è¿­ä»£ {iter_num + 1}/{iteration} - å˜åŒ–: {diff:.2e}")
            if diff < convergence_threshold:
                print(f"  âœ“ åœ¨è¿­ä»£ {iter_num + 1} è¾¾åˆ°æ”¶æ•›")
                break
        
        if (iter_num + 1) % 5 == 0:
            prev_row_sims = row_sims.copy()
    
    # æœ€ç»ˆæ•°å€¼æ£€æŸ¥
    final_min = np.min(row_sims)
    final_max = np.max(row_sims)
    final_mean = np.mean(row_sims)
    print(f"Sinkhornå®Œæˆç»Ÿè®¡:")
    print(f"  - å€¼èŒƒå›´: [{final_min:.2e}, {final_max:.2e}]")
    print(f"  - å¹³å‡å€¼: {final_mean:.2e}")
    print(f"  - NaNæ•°é‡: {np.sum(np.isnan(row_sims))}")
    print(f"  - Infæ•°é‡: {np.sum(np.isinf(row_sims))}")
    
    # é‡æ„ç»“æœ
    final_indices = indices  # ä¿æŒåŸå½¢çŠ¶ [size, safe_top_k]
    final_sims = row_sims.reshape(size, safe_top_k)
    
    print(f"ç»“æœç»´åº¦: indices={final_indices.shape}, sims={final_sims.shape}")
    
    # è®¡ç®—æ’å
    ranks = np.argsort(-final_sims, axis=1)
    
    # è¯„ä¼°
    wrong_list, right_list = [], []
    h1, h10, mrr = 0, 0, 0
    pos = np.zeros(np.max(right)+1, dtype=int)
    pos[right] = np.arange(len(right))
    
    print("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    for i in range(len(test_pair)):
        rank = np.where(pos[test_pair[i,1]] == final_indices[i,ranks[i]])[0]
        if len(rank) != 0:
            if rank[0] == 0:
                h1 += 1
                right_list.append(test_pair[i])
            else:
                wrong_list.append((test_pair[i], right[final_indices[i,ranks[i]][0]]))
            if rank[0] < 10:
                h10 += 1
            mrr += 1/(rank[0]+1) 
    
    print(f"ğŸ¯ ç¨³å®šç‰ˆSinkhorn Testç»“æœ:")
    print("Hits@1: %.3f Hits@10: %.3f MRR: %.3f\n"%(h1/len(test_pair), h10/len(test_pair), mrr/len(test_pair)))
    
    return right_list, wrong_list
