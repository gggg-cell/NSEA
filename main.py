import argparse
import copy

import numpy as np
import torch
from scipy.stats import rankdata
from framework import *
import time
from eval import sparse_acc,sparse_top_k, cur_max

# å¯¼å…¥æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
try:
    from smart_memory_cache import smart_cache
    CACHE_AVAILABLE = True
    print("âœ… æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿå·²åŠ è½½")
except ImportError:
    CACHE_AVAILABLE = False
    print("âš ï¸ æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿæœªæ‰¾åˆ°ï¼Œä½¿ç”¨åŸå§‹æ–¹å¼")

def get_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default='./mkdata/', help='path to dataset')
    parser.add_argument('--scale', type=str, default="large", help='scale of data')
    parser.add_argument('--step', type=int, default=3, help='step of data')
    parser.add_argument("--if_store", action='store_true', default=False)
    parser.add_argument("--result_folder", type=str, default='result/')
    parser.add_argument('--train_ratio', type=int, default=30)
    parser.add_argument("--top_k_corr", type=int, default=1)
    parser.add_argument("--k_partition", type=int, default=50)
    parser.add_argument("--backbone", type=str, default='duala')
    parser.add_argument("--method", type=str, default="ours",help="partition method")
    parser.add_argument("--device", type=str, default='cuda')
    parser.add_argument("--epoch", type=int, default=-1, help="number of epochs to train")
    parser.add_argument('--it_round', type=int, default=2)
    parser.add_argument('--round', type=int, default=10)
    parser.add_argument('--sbp', type=bool, default=False)
    parser.add_argument("--enhance", type=str, default='sinkhorn', help='mini-batch normalization')
    parser.add_argument("--save_folder", type=str, default='tmp/')
    parser.add_argument('--lang', type=str, default='fr', help='dataset language (fr, de)')
    parser.add_argument("--shuffle", type= bool, default=True, help="if shuffle data" )
    parser.add_argument("--src", type=int, default=0, help="which to train, 0 or 1")
    parser.add_argument("--norm", action="store_true", default=True, help="whether to normalize embeddings")
    parser.add_argument("--max_sinkhorn_sz", type=int, default=33000,
                        help="max matrix size to run Sinkhorn iteration"
                             ", if the matrix size is higher than this value"
                             ", it will calculate kNN search without normalizing to avoid OOM"
                             ", default is set for 33000^2 (for RTX3090)."
                             " could be set to higher value in case there is GPU with larger memory")
    return parser.parse_args()


global_arguments = get_arguments()
norm = global_arguments.norm
train_ratio = global_arguments.train_ratio
data = global_arguments.data
scale = global_arguments.scale
step = global_arguments.step
result_folder = global_arguments.result_folder
if_store = global_arguments.if_store
top_k_corr = global_arguments.top_k_corr
k_partiton = global_arguments.k_partition
backbone = global_arguments.backbone
max_sinkhorn_sz = global_arguments.max_sinkhorn_sz
method= global_arguments.method
lang = global_arguments.lang
device = global_arguments.device
enhance = global_arguments.enhance
epoch = global_arguments.epoch
n_semi_iter = global_arguments.it_round
sbp = global_arguments.sbp
save_folder = global_arguments.save_folder
shuffle_data = global_arguments.shuffle
src = global_arguments.src
if global_arguments.epoch == -1:
    train_epoch = \
        {'gcn-align': [2000] * n_semi_iter, 'rrea': [100] * n_semi_iter, 'dual-amn': [20] + [5] * (n_semi_iter - 1),
         'gcn-large': [50], 'dual-large': [20], 'rrea-large': [50], "duala": [20]}[
            backbone]

PHASE_PARTITION = 1  
PHASE_TRAINING = 2


def load_curr_objs(phase):
    try:
        return readobj(save_folder + get_suffix(phase))
    except:
        return readobj(save_folder + get_suffix(phase))

def get_suffix(phase, i=None):
    now = 'sim' if PHASE_TRAINING == phase else 'partition'
    if phase == PHASE_PARTITION:
        if i is not None:
            now += f"_{scale}_{method}_{lang}_shuffle{shuffle_data}_k{k_partiton}_ratio{train_ratio}_{i}.pkl"
        else:
            now += f"_{scale}_{method}_{lang}_shuffle{shuffle_data}_k{k_partiton}_ratio{train_ratio}.pkl"
        # now += ablation_args(sampler_methods, 'CST')
    elif phase == PHASE_TRAINING:
        if i is not None:
            now += f"_{scale}_{method}_{lang}_{backbone}_{enhance}_{train_ratio}_it{n_semi_iter}_{i}.pkl"
        else:
            now += f"_{scale}_{method}_{lang}_{backbone}_{enhance}_{train_ratio}_it{n_semi_iter}.pkl"
    else:
        raise NotImplementedError
    return now


def save_curr_objs(objs, phase,i=None):
    saveobj(objs, save_folder + get_suffix(phase, i))

def train(batch: AlignmentBatch, device: torch.device = 'cuda', **kwargs):
    # æ”¯æŒä¸¤ç§å‚æ•°ä¼ é€’æ–¹å¼ï¼šconfig_dictï¼ˆç¼“å­˜ä¼˜åŒ–ï¼‰å’Œargsï¼ˆåŸå§‹æ–¹å¼ï¼‰
    if 'config_dict' in kwargs:
        config_dict = kwargs['config_dict']
        it_round = config_dict.get('it_round', 1)
    else:
        args = kwargs['args']
        it_round = args.it_round
    
    if hasattr(batch, 'model'): 
        model = batch.model
        try:
            for it in range(it_round):
                model.train1step(train_epoch[0])
                if it < it_round - 1:
                    model.mraea_iteration()
            return model.get_curr_embeddings()

        except Exception as e:
            print('TF error', str(e))
            return None
        #  TODO
        pass
        # #  TODO
        # pass
    else:
        raise NotImplementedError
    

def step1_partition():
    ds = load_dataset(data, scale,lang, train_ratio=train_ratio*0.01, shuffle=shuffle_data)
    for i in range(global_arguments.round):
        partition = Partition(ds, k=k_partiton, src=src)
    # ours_nodes1, ours_nodes2 = partition.split_clusters(method='past')
        tmp1_nodes, tmp2_nodes, src_nodes, trg_nodes = partition.split_clusters(method=method)
        yield ds, tmp1_nodes, tmp2_nodes, src_nodes, trg_nodes

def step2_embeding(ds, framework, src_nodes, trg_nodes, round_idx, node_type):

    batch_sim_folder = save_folder + 'batch_sims/'
    import os
    os.makedirs(batch_sim_folder, exist_ok=True)
    
    curr_sim = None
    use_sinkhorn = 0
    batch_idx = 0
    align_loss_data = None  # å­˜å‚¨align_lossç»Ÿè®¡æ•°æ®
    
    for batch in framework.get_cluster_result(top_k_corr, backbone, src_nodes, trg_nodes, max_sinkhorn_sz):
        # æ”¶é›†align_lossç»Ÿè®¡æ•°æ®ï¼ˆæ¯ä¸ªbatchéƒ½æœ‰ç›¸åŒçš„align_loss_statsï¼‰
        if hasattr(batch, 'align_loss_stats') and align_loss_data is None:
            align_loss_data = batch.align_loss_stats.copy()
            align_loss_data['round'] = round_idx + 1
            align_loss_data['node_type'] = node_type
            align_loss_data['timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')
        
        # ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°ä¼ é€’
        if CACHE_AVAILABLE:
            # ä½¿ç”¨é…ç½®å­—å…¸æ›¿ä»£æ·±æ‹·è´ï¼Œå‡å°‘å†…å­˜å¼€é”€
            embed = train(batch, device, config_dict={'it_round': global_arguments.it_round})
        else:
            embed = train(batch, device, args=copy.deepcopy(global_arguments))
        
        if embed is None:
            continue
        left_embeding, right_embedding = tuple(embed)
        batch_sim, curr_use_sinkhorn = batch.create_batch_sim(left_embeding, right_embedding, enhance, ds.size(),
                                                             norm=norm, return_use_sinkhorn=True)
        if batch_sim is None:
            continue
        use_sinkhorn += curr_use_sinkhorn
        print('Total sinkhorn=', use_sinkhorn)
        
        if curr_sim is None:
            curr_sim = batch_sim
        else:
            curr_sim = curr_sim + batch_sim        
            curr_sim = curr_sim.coalesce()

        del batch_sim
        torch.cuda.empty_cache()
    result = sparse_acc(curr_sim, ds.ill(ds.test, 'cuda'))
    print(f'Batch {batch_idx} accumulated acc is', result)
    return curr_sim, align_loss_data

def run():
    start_total = time.time()
    torch.cuda.set_device(0)
    
    # åˆ›å»ºæ—¶é—´ç»Ÿè®¡å­—å…¸
    time_stats = {
        'step1_partition': 0,
        'step2_embedding': 0, 
        'step3_evaluation': 0,
        'total_time': 0
    }
    
    # eval_large()
    if step <= 1:  
        step1_start = time.time()
        print("å¼€å§‹æ‰§è¡Œæ­¥éª¤1: åˆ†åŒº...")
        for index, nodes in enumerate(step1_partition()):
            save_curr_objs(nodes, PHASE_PARTITION,index)
        step1_end = time.time()
        time_stats['step1_partition'] = step1_end - step1_start
        print(f"æ­¥éª¤1å®Œæˆï¼Œè€—æ—¶: {time_stats['step1_partition']:.2f} ç§’")

    if step <= 2:  # å¦‚æœstepä¸º1æˆ–2ï¼Œæ‰§è¡Œæ­¥éª¤2
        step2_start = time.time()
        print("å¼€å§‹æ‰§è¡Œæ­¥éª¤2: åµŒå…¥è®­ç»ƒ...")
        
        # åˆ›å»ºalign_lossè®°å½•åˆ—è¡¨
        align_loss_records = []
        
        for i in range(global_arguments.round):
            print(f"\n--- å¤„ç†ç¬¬ {i+1}/{global_arguments.round} è½® ---")

            ds, tmp1_nodes, tmp2_nodes, src_nodes, trg_nodes = readobj(save_folder + get_suffix(PHASE_PARTITION,i))
            framework = LargepartitonFramework(ds, device='cuda', k=k_partiton, src=0, )
            
            # æ‰§è¡ŒåµŒå…¥è®­ç»ƒï¼ˆèŠ‚ç‚¹æ•°æ®é€šè¿‡ç¼“å­˜å‡å°‘ä¼ é€’å¼€é”€ï¼‰
            sim1, align_loss_data1 = step2_embeding(ds, framework, tmp1_nodes, tmp2_nodes, i, 'tmp')
            sim2, align_loss_data2 = step2_embeding(ds, framework, src_nodes, trg_nodes, i, 'src')
            
            # æ”¶é›†align_lossæ•°æ®
            if align_loss_data1:
                align_loss_records.append(align_loss_data1)
            if align_loss_data2:
                align_loss_records.append(align_loss_data2)
            
            save_curr_objs((framework,sim1,sim2), PHASE_TRAINING, i)
            del sim1, sim2
            torch.cuda.empty_cache()
        
        # ä¿å­˜align_lossæ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶
        if align_loss_records:
            import json
            import os
            from datetime import datetime
            
            # ç¡®ä¿ç»“æœæ–‡ä»¶å¤¹å­˜åœ¨
            os.makedirs(result_folder, exist_ok=True)
            
            # åˆ›å»ºè¯¦ç»†çš„align_lossæŠ¥å‘Š
            align_loss_report = {
                'experiment_info': {
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'scale': scale,
                    'method': method,
                    'lang': lang,
                    'backbone': backbone,
                    'train_ratio': train_ratio,
                    'k_partition': k_partiton,
                    'total_rounds': global_arguments.round,
                    'device': device
                },
                'align_loss_records': align_loss_records,
                'summary': {
                    'total_records': len(align_loss_records),
                    'avg_align_loss_percentage': sum([r['align_loss_percentage'] for r in align_loss_records]) / len(align_loss_records) if align_loss_records else 0,
                    'avg_overlap_percentage': sum([r['overlap_percentage'] for r in align_loss_records]) / len(align_loss_records) if align_loss_records else 0,
                    'avg_ent_loss1': sum([r['ent_loss1'] for r in align_loss_records]) / len(align_loss_records) if align_loss_records else 0,
                    'avg_ent_loss2': sum([r['ent_loss2'] for r in align_loss_records]) / len(align_loss_records) if align_loss_records else 0
                }
            }
            
            # ä¿å­˜JSONæ ¼å¼çš„align_lossè®°å½•
            align_loss_json_file = os.path.join(result_folder, f'align_loss_records_{scale}_{method}_{lang}.json')
            with open(align_loss_json_file, 'w', encoding='utf-8') as f:
                json.dump(align_loss_report, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜ç®€å•çš„æ–‡æœ¬æ ¼å¼align_lossè®°å½•
            align_loss_txt_file = os.path.join(result_folder, f'align_loss_report_{scale}_{method}_{lang}.txt')
            with open(align_loss_txt_file, 'w', encoding='utf-8') as f:
                f.write("Align Loss æŠ¥å‘Š\n")
                f.write("="*70 + "\n")
                f.write(f"å®éªŒæ—¶é—´: {align_loss_report['experiment_info']['timestamp']}\n")
                f.write(f"æ•°æ®è§„æ¨¡: {scale}\n")
                f.write(f"æ–¹æ³•: {method}\n")
                f.write(f"è¯­è¨€: {lang}\n")
                f.write(f"éª¨å¹²ç½‘ç»œ: {backbone}\n")
                f.write(f"è®­ç»ƒæ¯”ä¾‹: {train_ratio}%\n")
                f.write(f"åˆ†åŒºæ•°: {k_partiton}\n")
                f.write(f"æ€»è½®æ¬¡: {global_arguments.round}\n")
                f.write(f"è®¾å¤‡: {device}\n")
                f.write("-" * 70 + "\n")
                f.write("å„è½®æ¬¡Align Lossè¯¦æƒ…:\n")
                for i, record in enumerate(align_loss_records):
                    f.write(f"\nè®°å½• {i+1} (ç¬¬{record['round']}è½®, {record['node_type']}èŠ‚ç‚¹):\n")
                    f.write(f"  æ—¶é—´æˆ³: {record['timestamp']}\n")
                    f.write(f"  Align Lossç™¾åˆ†æ¯”: {record['align_loss_percentage']:.4f}%\n")
                    f.write(f"  é‡å ç™¾åˆ†æ¯”: {record['overlap_percentage']:.4f}%\n")
                    f.write(f"  é…å¯¹æ•°é‡: {record['pair_count']}\n")
                    f.write(f"  æ˜ å°„æ€»æ•°: {record['mapping_count']}\n")
                    f.write(f"  æ€»å’Œ: {record['total_sum']}\n")
                    f.write(f"  å®ä½“æŸå¤±1: {record['ent_loss1']}\n")
                    f.write(f"  å®ä½“æŸå¤±2: {record['ent_loss2']}\n")
                    f.write(f"  èŠ‚ç‚¹æ•°1: {record['has_nodes1_count']}/{record['total_ent1']}\n")
                    f.write(f"  èŠ‚ç‚¹æ•°2: {record['has_nodes2_count']}/{record['total_ent2']}\n")
                f.write("-" * 70 + "\n")
                f.write("ç»Ÿè®¡æ‘˜è¦:\n")
                f.write(f"æ€»è®°å½•æ•°: {align_loss_report['summary']['total_records']}\n")
                f.write(f"å¹³å‡Align Lossç™¾åˆ†æ¯”: {align_loss_report['summary']['avg_align_loss_percentage']:.4f}%\n")
                f.write(f"å¹³å‡é‡å ç™¾åˆ†æ¯”: {align_loss_report['summary']['avg_overlap_percentage']:.4f}%\n")
                f.write(f"å¹³å‡å®ä½“æŸå¤±1: {align_loss_report['summary']['avg_ent_loss1']:.2f}\n")
                f.write(f"å¹³å‡å®ä½“æŸå¤±2: {align_loss_report['summary']['avg_ent_loss2']:.2f}\n")
                f.write("="*70 + "\n")
            
            # ä¿å­˜CSVæ ¼å¼çš„align_lossè®°å½•
            align_loss_csv_file = os.path.join(result_folder, f'align_loss_data_{scale}_{method}_{lang}.csv')
            with open(align_loss_csv_file, 'w', encoding='utf-8') as f:
                f.write("è½®æ¬¡,èŠ‚ç‚¹ç±»å‹,Align_Lossç™¾åˆ†æ¯”,é‡å ç™¾åˆ†æ¯”,é…å¯¹æ•°é‡,æ˜ å°„æ€»æ•°,æ€»å’Œ,å®ä½“æŸå¤±1,å®ä½“æŸå¤±2,æ—¶é—´æˆ³\n")
                for record in align_loss_records:
                    f.write(f"{record['round']},{record['node_type']},{record['align_loss_percentage']:.6f},"
                           f"{record['overlap_percentage']:.6f},{record['pair_count']},{record['mapping_count']},"
                           f"{record['total_sum']},{record['ent_loss1']},{record['ent_loss2']},{record['timestamp']}\n")
            
            print(f"\nAlign Lossè®°å½•å·²ä¿å­˜åˆ°:")
            print(f"è¯¦ç»†æŠ¥å‘Š (JSON): {align_loss_json_file}")
            print(f"æ–‡æœ¬æŠ¥å‘Š (TXT):  {align_loss_txt_file}")
            print(f"æ•°æ®æ–‡ä»¶ (CSV):  {align_loss_csv_file}")
        
        step2_end = time.time()
        time_stats['step2_embedding'] = step2_end - step2_start
        print(f"æ­¥éª¤2å®Œæˆï¼Œè€—æ—¶: {time_stats['step2_embedding']:.2f} ç§’")

    if step <= 3:  # å¦‚æœstepä¸º1ã€2æˆ–3ï¼Œæ‰§è¡Œæ­¥éª¤3
        step3_start = time.time()
        print("å¼€å§‹æ‰§è¡Œæ­¥éª¤3: æœ€ç»ˆè¯„ä¼°...")
        sum_matrix = None
        framework = None
        
        # åˆ›å»ºç²¾ç¡®åº¦è®°å½•åˆ—è¡¨
        accuracy_records = []
        
        for i in range(global_arguments.round):
            try:
                framework, sim1, sim2 = readobj(save_folder + get_suffix(PHASE_TRAINING, i))
                framework.device = device
                print(f'æˆåŠŸåŠ è½½ç¬¬{i}è½®çš„frameworkå’Œç›¸ä¼¼åº¦çŸ©é˜µ')
            except Exception as e:
                print(f'åŠ è½½ç¬¬{i}è½®æ•°æ®å¤±è´¥: {e}')
                continue
            
            # ç´¯åŠ å½“å‰è½®æ¬¡çš„ç›¸ä¼¼åº¦çŸ©é˜µ
            round_sum = None
            
            # å¤„ç†sim1ï¼ˆtmpèŠ‚ç‚¹çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼‰
            if sim1 is not None:
                sim1 = sim1.to(device)
                round_sum = sim1
                print(f'ç¬¬{i}è½®æ·»åŠ sim1ï¼Œå¤§å°: {sim1.size()}')
                torch.cuda.empty_cache()

            # å¤„ç†sim2ï¼ˆsrcèŠ‚ç‚¹çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼‰
            if sim2 is not None:
                sim2 = sim2.to(device)
                if round_sum is None:
                    round_sum = sim2
                else:
                    round_sum = round_sum + sim2
                    round_sum = round_sum.coalesce()
                torch.cuda.empty_cache()
                print(f'ç¬¬{i}è½®æ·»åŠ sim2ï¼Œå¤§å°: {sim2.size()}')
            
            # å°†å½“å‰è½®æ¬¡çš„ç´¯åŠ ç»“æœæ·»åŠ åˆ°æ€»å’Œä¸­
            if round_sum is not None:
                if sum_matrix is None:
                    sum_matrix = round_sum
                else:
                    # åœ¨GPUä¸Šç´¯åŠ çŸ©é˜µ
                    sum_matrix = sum_matrix + round_sum
                    sum_matrix = sum_matrix.coalesce()
                
                # è¯„ä¼°å½“å‰ç»“æœå¹¶è®°å½•ç²¾ç¡®åº¦
                print(f'ç¬¬{i}è½®ç´¯åŠ åè¯„ä¼°ç»“æœ:')
                eval_result = framework.eval_sim(sum_matrix)
                
                # æå–æ‰€æœ‰hitsæŒ‡æ ‡
                hits_metrics = extract_hits_metrics(eval_result)
                
                # è®°å½•å½“å‰è½®æ¬¡çš„ç²¾ç¡®åº¦ï¼ˆåŒ…å«æ‰€æœ‰hitsæŒ‡æ ‡å’ŒMRRï¼‰
                round_record = {
                    'round': i + 1,
                    'hits@1': hits_metrics['hits@1'],
                    'hits@5': hits_metrics['hits@5'], 
                    'hits@10': hits_metrics['hits@10'],
                    'MRR': hits_metrics['MRR'],
                    'accuracy': hits_metrics['hits@1'],  # ä¿æŒå‘åå…¼å®¹
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                accuracy_records.append(round_record)
                
                print(f'ç¬¬{i+1}è½®ç´¯åŠ ç²¾ç¡®åº¦: Hits@1={hits_metrics["hits@1"]:.4f}, Hits@5={hits_metrics["hits@5"]:.4f}, Hits@10={hits_metrics["hits@10"]:.4f}, MRR={hits_metrics["MRR"]:.4f}')
                
                # é‡Šæ”¾ä¸å†éœ€è¦çš„GPUå†…å­˜
                del sim1, sim2, round_sum
                torch.cuda.empty_cache()
            else:
                print(f'ç¬¬{i}è½®æ²¡æœ‰æœ‰æ•ˆçš„ç›¸ä¼¼åº¦çŸ©é˜µ')
                # å³ä½¿æ²¡æœ‰æœ‰æ•ˆçŸ©é˜µï¼Œä¹Ÿè®°å½•ä¸€ä¸ª0ç²¾ç¡®åº¦
                round_record = {
                    'round': i + 1,
                    'hits@1': 0.0,
                    'hits@5': 0.0,
                    'hits@10': 0.0,
                    'MRR': 0.0,
                    'accuracy': 0.0,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                accuracy_records.append(round_record)
        
        # æœ€ç»ˆè¯„ä¼°
        final_hits_metrics = {'hits@1': 0.0, 'hits@5': 0.0, 'hits@10': 0.0, 'MRR': 0.0}
        
        if sum_matrix is not None and framework is not None:
            print('æ‰€æœ‰è½®æ¬¡ç´¯åŠ å®Œæˆï¼Œæœ€ç»ˆè¯„ä¼°ç»“æœ:')
            final_result = framework.eval_sim(sum_matrix)
            # æå–æœ€ç»ˆçš„hitsæŒ‡æ ‡
            final_hits_metrics = extract_hits_metrics(final_result)
            test_src_indices = np.array([pair[0] for pair in framework.ds.test])
            test_tgt_indices = np.array([pair[1]+len(framework.ds.ent1) for pair in framework.ds.test])
            test_pair = np.column_stack([test_src_indices, test_tgt_indices])
            
            # # ä½¿ç”¨evaluation.pyä¸­æ›´æ–°åçš„testå‡½æ•°å¤„ç†ç¨€ç–ç›¸ä¼¼åº¦çŸ©é˜µ
            # # ä½¿ç”¨è¾ƒå°çš„top_kå€¼ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
            # framework.eval_sim(sum_matrix, top_k=100, iteration=15, temperature=1)

            print(f'âœ… æœ€ç»ˆè¯„ä¼°ç»“æœ:')
            print(f'   Hits@1:  {final_hits_metrics["hits@1"]:.6f}')
            print(f'   Hits@5:  {final_hits_metrics["hits@5"]:.6f}')
            print(f'   Hits@10: {final_hits_metrics["hits@10"]:.6f}')
            print(f'   MRR:     {final_hits_metrics["MRR"]:.6f}')
        else:
            print('æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ç›¸ä¼¼åº¦çŸ©é˜µæˆ–framework')
            print('ä½¿ç”¨é»˜è®¤çš„0å€¼ä½œä¸ºæœ€ç»ˆç»“æœ')
        
        # ä¿å­˜ç²¾ç¡®åº¦è®°å½•åˆ°æœ¬åœ°æ–‡ä»¶
        import json
        import os
        from datetime import datetime
        
        # ç¡®ä¿ç»“æœæ–‡ä»¶å¤¹å­˜åœ¨
        os.makedirs(result_folder, exist_ok=True)
        
        # åˆ›å»ºè¯¦ç»†çš„ç²¾ç¡®åº¦æŠ¥å‘Š
        acc_report = {
            'experiment_info': {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'scale': scale,
                'method': method,
                'lang': lang,
                'backbone': backbone,
                'train_ratio': train_ratio,
                'k_partition': k_partiton,
                'total_rounds': global_arguments.round,
                'device': device
            },
            'round_accuracies': accuracy_records,
            'final_metrics': {
                'hits@1': float(final_hits_metrics['hits@1']),
                'hits@5': float(final_hits_metrics['hits@5']),
                'hits@10': float(final_hits_metrics['hits@10']),
                'MRR': float(final_hits_metrics['MRR'])
            },
            'final_accuracy': float(final_hits_metrics['hits@1']),  # ä¿æŒå‘åå…¼å®¹
            'summary': {
                'hits@1': {
                    'max': max([r['hits@1'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'min': min([r['hits@1'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'avg': sum([r['hits@1'] for r in accuracy_records]) / len(accuracy_records) if accuracy_records else 0.0
                },
                'hits@5': {
                    'max': max([r['hits@5'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'min': min([r['hits@5'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'avg': sum([r['hits@5'] for r in accuracy_records]) / len(accuracy_records) if accuracy_records else 0.0
                },
                'hits@10': {
                    'max': max([r['hits@10'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'min': min([r['hits@10'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'avg': sum([r['hits@10'] for r in accuracy_records]) / len(accuracy_records) if accuracy_records else 0.0
                },
                'MRR': {
                    'max': max([r['MRR'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'min': min([r['MRR'] for r in accuracy_records]) if accuracy_records else 0.0,
                    'avg': sum([r['MRR'] for r in accuracy_records]) / len(accuracy_records) if accuracy_records else 0.0
                },
                # ä¿æŒå‘åå…¼å®¹
                'max_accuracy': max([r['hits@1'] for r in accuracy_records]) if accuracy_records else 0.0,
                'min_accuracy': min([r['hits@1'] for r in accuracy_records]) if accuracy_records else 0.0,
                'avg_accuracy': sum([r['hits@1'] for r in accuracy_records]) / len(accuracy_records) if accuracy_records else 0.0
            }
        }
        
        # ä¿å­˜JSONæ ¼å¼çš„ç²¾ç¡®åº¦è®°å½•
        acc_json_file = os.path.join(result_folder, f'accuracy_records_{scale}_{method}_{lang}.json')
        with open(acc_json_file, 'w', encoding='utf-8') as f:
            json.dump(acc_report, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç®€å•çš„æ–‡æœ¬æ ¼å¼ç²¾ç¡®åº¦è®°å½•
        acc_txt_file = os.path.join(result_folder, f'accuracy_report_{scale}_{method}_{lang}.txt')
        with open(acc_txt_file, 'w', encoding='utf-8') as f:
            f.write("ç²¾ç¡®åº¦æŠ¥å‘Š (åŒ…å«Hits@1/5/10å’ŒMRRæŒ‡æ ‡)\n")
            f.write("="*70 + "\n")
            f.write(f"å®éªŒæ—¶é—´: {acc_report['experiment_info']['timestamp']}\n")
            f.write(f"æ•°æ®è§„æ¨¡: {scale}\n")
            f.write(f"æ–¹æ³•: {method}\n")
            f.write(f"è¯­è¨€: {lang}\n")
            f.write(f"éª¨å¹²ç½‘ç»œ: {backbone}\n")
            f.write(f"è®­ç»ƒæ¯”ä¾‹: {train_ratio}%\n")
            f.write(f"åˆ†åŒºæ•°: {k_partiton}\n")
            f.write(f"æ€»è½®æ¬¡: {global_arguments.round}\n")
            f.write(f"è®¾å¤‡: {device}\n")
            f.write("-" * 70 + "\n")
            f.write("æœ€ç»ˆè¯„ä¼°ç»“æœ:\n")
            f.write(f"  Hits@1:  {acc_report['final_metrics']['hits@1']:.6f}\n")
            f.write(f"  Hits@5:  {acc_report['final_metrics']['hits@5']:.6f}\n")
            f.write(f"  Hits@10: {acc_report['final_metrics']['hits@10']:.6f}\n")
            f.write(f"  MRR:     {acc_report['final_metrics']['MRR']:.6f}\n")
            f.write("-" * 70 + "\n")
            f.write("å„è½®æ¬¡ç²¾ç¡®åº¦:\n")
            f.write(f"{'è½®æ¬¡':>4} {'Hits@1':>10} {'Hits@5':>10} {'Hits@10':>11} {'MRR':>10} {'æ—¶é—´æˆ³':>20}\n")
            f.write("-" * 70 + "\n")
            for record in accuracy_records:
                f.write(f"{record['round']:>4} {record['hits@1']:>10.6f} {record['hits@5']:>10.6f} {record['hits@10']:>11.6f} {record['MRR']:>10.6f} {record['timestamp']:>20}\n")
            f.write("-" * 70 + "\n")
            f.write("ç»Ÿè®¡æ‘˜è¦:\n")
            f.write("Hits@1:\n")
            f.write(f"  æœ€é«˜: {acc_report['summary']['hits@1']['max']:.6f}\n")
            f.write(f"  æœ€ä½: {acc_report['summary']['hits@1']['min']:.6f}\n") 
            f.write(f"  å¹³å‡: {acc_report['summary']['hits@1']['avg']:.6f}\n")
            f.write("Hits@5:\n")
            f.write(f"  æœ€é«˜: {acc_report['summary']['hits@5']['max']:.6f}\n")
            f.write(f"  æœ€ä½: {acc_report['summary']['hits@5']['min']:.6f}\n")
            f.write(f"  å¹³å‡: {acc_report['summary']['hits@5']['avg']:.6f}\n")
            f.write("Hits@10:\n")
            f.write(f"  æœ€é«˜: {acc_report['summary']['hits@10']['max']:.6f}\n")
            f.write(f"  æœ€ä½: {acc_report['summary']['hits@10']['min']:.6f}\n")
            f.write(f"  å¹³å‡: {acc_report['summary']['hits@10']['avg']:.6f}\n")
            f.write("MRR:\n")
            f.write(f"  æœ€é«˜: {acc_report['summary']['MRR']['max']:.6f}\n")
            f.write(f"  æœ€ä½: {acc_report['summary']['MRR']['min']:.6f}\n")
            f.write(f"  å¹³å‡: {acc_report['summary']['MRR']['avg']:.6f}\n")
            f.write("="*70 + "\n")
        
        # ä¿å­˜CSVæ ¼å¼çš„ç²¾ç¡®åº¦è®°å½•ï¼ˆä¾¿äºExcelæ‰“å¼€ï¼‰
        acc_csv_file = os.path.join(result_folder, f'accuracy_data_{scale}_{method}_{lang}.csv')
        with open(acc_csv_file, 'w', encoding='utf-8') as f:
            f.write("è½®æ¬¡,Hits@1,Hits@5,Hits@10,MRR,æ—¶é—´æˆ³\n")
            for record in accuracy_records:
                f.write(f"{record['round']},{record['hits@1']:.6f},{record['hits@5']:.6f},{record['hits@10']:.6f},{record['MRR']:.6f},{record['timestamp']}\n")
            f.write(f"æœ€ç»ˆ,{acc_report['final_metrics']['hits@1']:.6f},{acc_report['final_metrics']['hits@5']:.6f},{acc_report['final_metrics']['hits@10']:.6f},{acc_report['final_metrics']['MRR']:.6f},{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        print(f"\nç²¾ç¡®åº¦è®°å½•å·²ä¿å­˜åˆ°:")
        print(f"è¯¦ç»†æŠ¥å‘Š (JSON): {acc_json_file}")
        print(f"æ–‡æœ¬æŠ¥å‘Š (TXT):  {acc_txt_file}")
        print(f"æ•°æ®æ–‡ä»¶ (CSV):  {acc_csv_file}")
        
        step3_end = time.time()
        time_stats['step3_evaluation'] = step3_end - step3_start
        print(f"æ­¥éª¤3å®Œæˆï¼Œè€—æ—¶: {time_stats['step3_evaluation']:.2f} ç§’")

    end_total = time.time()
    time_stats['total_time'] = end_total - start_total
    
    # æ‰“å°æ—¶é—´ç»Ÿè®¡
    print("\n" + "="*50)
    print("æ—¶é—´ç»Ÿè®¡æŠ¥å‘Š:")
    print("="*50)
    print(f"æ­¥éª¤1 (åˆ†åŒº):     {time_stats['step1_partition']:.2f} ç§’")
    print(f"æ­¥éª¤2 (åµŒå…¥è®­ç»ƒ): {time_stats['step2_embedding']:.2f} ç§’") 
    print(f"æ­¥éª¤3 (æœ€ç»ˆè¯„ä¼°): {time_stats['step3_evaluation']:.2f} ç§’")
    print(f"æ€»è€—æ—¶:          {time_stats['total_time']:.2f} ç§’")
    print("="*50)
    
    # ä¿å­˜æ—¶é—´ç»Ÿè®¡åˆ°æœ¬åœ°æ–‡ä»¶
    import json
    import os
    from datetime import datetime
    
    # ç¡®ä¿ç»“æœæ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(result_folder, exist_ok=True)
    
    # æ·»åŠ æ›´å¤šè¯¦ç»†ä¿¡æ¯åˆ°ç»Ÿè®¡ä¸­
    detailed_stats = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'parameters': {
            'scale': scale,
            'method': method,
            'lang': lang,
            'backbone': backbone,
            'train_ratio': train_ratio,
            'k_partition': k_partiton,
            'rounds': global_arguments.round,
            'device': device
        },
        'timing': time_stats
    }
    
    # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†ç»Ÿè®¡
    json_file = os.path.join(result_folder, f'timing_stats_{scale}_{method}_{lang}.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_stats, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜ç®€å•çš„æ–‡æœ¬æ ¼å¼ç»Ÿè®¡
    txt_file = os.path.join(result_folder, f'timing_report_{scale}_{method}_{lang}.txt')
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("æ—¶é—´ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("="*50 + "\n")
        f.write(f"è¿è¡Œæ—¶é—´: {detailed_stats['timestamp']}\n")
        f.write(f"æ•°æ®è§„æ¨¡: {scale}\n")
        f.write(f"æ–¹æ³•: {method}\n") 
        f.write(f"è¯­è¨€: {lang}\n")
        f.write(f"éª¨å¹²ç½‘ç»œ: {backbone}\n")
        f.write(f"è®­ç»ƒæ¯”ä¾‹: {train_ratio}%\n")
        f.write(f"åˆ†åŒºæ•°: {k_partiton}\n")
        f.write(f"è½®æ¬¡: {global_arguments.round}\n")
        f.write(f"è®¾å¤‡: {device}\n")
        f.write("-" * 50 + "\n")
        f.write(f"æ­¥éª¤1 (åˆ†åŒº):     {time_stats['step1_partition']:.2f} ç§’\n")
        f.write(f"æ­¥éª¤2 (åµŒå…¥è®­ç»ƒ): {time_stats['step2_embedding']:.2f} ç§’\n")
        f.write(f"æ­¥éª¤3 (æœ€ç»ˆè¯„ä¼°): {time_stats['step3_evaluation']:.2f} ç§’\n")
        f.write(f"æ€»è€—æ—¶:          {time_stats['total_time']:.2f} ç§’\n")
        f.write("="*50 + "\n")
    
    print(f"\næ—¶é—´ç»Ÿè®¡å·²ä¿å­˜åˆ°:")
    print(f"è¯¦ç»†ç»Ÿè®¡ (JSON): {json_file}")
    print(f"ç®€å•æŠ¥å‘Š (TXT):  {txt_file}")
    
    # è¾“å‡ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯å’Œæ¸…ç†
    if CACHE_AVAILABLE:
        print("\n" + "="*60)
        print("ğŸš€ æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        smart_cache.print_stats()
        
        # æœ€ç»ˆæ¸…ç†æ‰€æœ‰ç¼“å­˜
        print("\nğŸ§¹ æ¸…ç†æ‰€æœ‰ç¼“å­˜...")
        smart_cache.clear_all_cache()
        print("âœ… ç¼“å­˜æ¸…ç†å®Œæˆ")
    
    return time_stats
        
def extract_hits_metrics(eval_result, default_value=0.0):
    """
    ä»è¯„ä¼°ç»“æœä¸­æå–hits@1, hits@5, hits@10, MRRæŒ‡æ ‡
    
    Args:
        eval_result: è¯„ä¼°å‡½æ•°çš„è¿”å›ç»“æœ
        default_value: é»˜è®¤å€¼ï¼ˆå½“æ— æ³•æå–æ—¶ï¼‰
        
    Returns:
        dict: åŒ…å«hits@1, hits@5, hits@10, MRRçš„å­—å…¸
    """
    hits_metrics = {
        'hits@1': default_value,
        'hits@5': default_value, 
        'hits@10': default_value,
        'MRR': default_value
    }
    
    try:
        if isinstance(eval_result, tuple) and len(eval_result) >= 2:
            acc_result = eval_result[1]
            if isinstance(acc_result, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥æå–hitsæŒ‡æ ‡å’ŒMRR
                for key in ['hits@1', 'hits@5', 'hits@10']:
                    if key in acc_result:
                        hits_metrics[key] = float(acc_result[key])
                
                # å¤„ç†MRRæŒ‡æ ‡ï¼ˆå¯èƒ½çš„é”®åï¼šMRR, mrr, mean_reciprocal_rankï¼‰
                for mrr_key in ['MRR', 'mrr', 'mean_reciprocal_rank']:
                    if mrr_key in acc_result:
                        hits_metrics['MRR'] = float(acc_result[mrr_key])
                        break
                
                print(f"âœ… æå–åˆ°å®Œæ•´æŒ‡æ ‡: {hits_metrics}")
            else:
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œå°†å•ä¸ªå€¼èµ‹ç»™hits@1
                hits_metrics['hits@1'] = float(acc_result)
                print(f"ğŸ“Š ä½¿ç”¨å•ä¸€ç²¾ç¡®åº¦å€¼ä½œä¸ºhits@1: {hits_metrics['hits@1']}")
        elif isinstance(eval_result, (int, float)):
            hits_metrics['hits@1'] = float(eval_result)
            print(f"ğŸ“Š ä½¿ç”¨æ•°å€¼ç»“æœä½œä¸ºhits@1: {hits_metrics['hits@1']}")
        else:
            print(f"âš ï¸ æ— æ³•è¯†åˆ«çš„è¯„ä¼°ç»“æœæ ¼å¼: {type(eval_result)}")
    except Exception as e:
        print(f"âŒ æå–æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
    
    return hits_metrics


def align_loss():
    with open(f'align_loss_result.csv', 'w') as f:
        f.write('data,lang,train_ratio,align_loss,ent_loss1,ent_loss2\n')
    for data in [ 'DBpedia1M']:
        if data == 'mkdata':
            scale = 'large'
        else:
            scale = 'largegnn'
        for lang in ['fr', 'de']:
            for train_ratio in range(10, 31, 5):
                ds = load_dataset(data, scale,lang, train_ratio=train_ratio*0.01, shuffle=shuffle_data)
                partition = Partition(ds, k=k_partiton, src=src)
                result = partition.split_clusters(method="align_loss")
                #å°†æŸå¤±ä¿å­˜åˆ°æ–‡ä»¶ä¸­ 
                with open(f'align_loss_result.csv', 'a') as f:
                    f.write(f'{data},{lang},{train_ratio},{result["align_loss"]},{result["ent_loss1"]},{result["ent_loss2"]}\n')

if __name__ == '__main__':
    align_loss();
